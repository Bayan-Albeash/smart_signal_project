"""
XGBoost Predictor - ML model for predicting tower loads and optimizing distribution
Integrated with Google Cloud Vertex AI for enhanced ML capabilities
"""

import random
import os
import json
import logging
import numpy as np
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta

# Google Cloud AI Platform imports
from google.cloud import aiplatform
from google.cloud import storage
import google.generativeai as genai
from google.auth import default
from google.api_core import exceptions

logger = logging.getLogger(__name__)

class XGBoostPredictor:
    """XGBoost-based predictor for cellular tower load optimization with Vertex AI integration"""
    
    def __init__(self, 
                 model_path: str = None,
                 project_id: str = None,
                 location: str = "us-central1",
                 use_vertex_ai: bool = True):
        
        self.feature_columns = [
            'current_load', 'capacity', 'time_of_day', 'day_of_week',
            'historical_avg_load', 'user_density', 'tower_age',
            'coverage_area', 'nearby_towers_count', 'operator_type'
        ]
        
        # Configuration
        self.model_path = model_path or '../../ml-models/tower_predictor.pkl'
        self.project_id = project_id or os.environ.get('GOOGLE_CLOUD_PROJECT')
        self.location = location
        self.use_vertex_ai = use_vertex_ai and self.project_id
        
        # Initialize components
        self.model = None
        self.vertex_endpoint = None
        self.storage_client = None
        self.gemini_model = None
        
        # Initialize services
        self._initialize_services()
        
        print("âœ… XGBoost Predictor initialized with Vertex AI integration")
    
    def _initialize_services(self):
        """Initialize Google Cloud services"""
        if not self.use_vertex_ai:
            print("âš ï¸  Running in local mode without Vertex AI")
            return
            
        try:
            # Initialize credentials
            credentials, project = default()
            
            # Initialize Vertex AI
            aiplatform.init(
                project=self.project_id,
                location=self.location,
                credentials=credentials
            )
            
            # Initialize Cloud Storage
            self.storage_client = storage.Client(
                project=self.project_id,
                credentials=credentials
            )
            
            # Initialize Gemini AI
            if os.environ.get('GOOGLE_API_KEY'):
                genai.configure(api_key=os.environ.get('GOOGLE_API_KEY'))
                self.gemini_model = genai.GenerativeModel('gemini-pro')
                
            logger.info("âœ… Google Cloud services initialized successfully")
            
        except Exception as e:
            logger.error(f"âŒ Failed to initialize Google Cloud services: {e}")
            self.use_vertex_ai = False
    
    async def deploy_model_to_vertex_ai(self, 
                                      model_display_name: str = "smart-signal-xgboost",
                                      endpoint_display_name: str = "smart-signal-endpoint"):
        """Deploy XGBoost model to Vertex AI"""
        if not self.use_vertex_ai:
            raise ValueError("Vertex AI not configured")
            
        try:
            # Upload model to Vertex AI Model Registry
            model = aiplatform.Model.upload(
                display_name=model_display_name,
                artifact_uri=f"gs://{self.project_id}-ml-models/xgboost/",
                serving_container_image_uri="gcr.io/cloud-aiplatform/prediction/xgboost-cpu.1-4:latest",
                description="Smart Signal XGBoost model for tower load prediction"
            )
            
            # Deploy model to endpoint
            endpoint = model.deploy(
                deployed_model_display_name=endpoint_display_name,
                machine_type="n1-standard-2",
                min_replica_count=1,
                max_replica_count=3,
                accelerator_type=None,
                accelerator_count=0
            )
            
            self.vertex_endpoint = endpoint
            logger.info(f"âœ… Model deployed to Vertex AI endpoint: {endpoint.resource_name}")
            return endpoint
            
        except Exception as e:
            logger.error(f"âŒ Failed to deploy model to Vertex AI: {e}")
            raise
    
    def predict_tower_loads_vertex_ai(self, towers_data: List[Dict[str, Any]]) -> Dict[int, float]:
        """Predict using Vertex AI endpoint"""
        if not self.vertex_endpoint:
            return self.predict_tower_loads_local(towers_data)
            
        try:
            # Prepare instances for prediction
            instances = []
            for tower_data in towers_data:
                instance = self._prepare_features(tower_data)
                instances.append(instance)
            
            # Make predictions using Vertex AI
            predictions = self.vertex_endpoint.predict(instances=instances)
            
            # Process results
            results = {}
            for i, tower_data in enumerate(towers_data):
                tower_id = tower_data.get('id', i)
                predicted_load = predictions.predictions[i][0] if predictions.predictions else 0
                results[tower_id] = max(10, predicted_load)
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Vertex AI prediction failed, falling back to local: {e}")
            return self.predict_tower_loads_local(towers_data)
    
    def _prepare_features(self, tower_data: Dict[str, Any]) -> List[float]:
        """Prepare features for ML prediction"""
        current_time = datetime.now()
        
        features = [
            tower_data.get('current_load', 100),
            tower_data.get('capacity', 200),
            current_time.hour,
            current_time.weekday(),
            tower_data.get('historical_avg_load', 80),
            tower_data.get('user_density', 50),
            tower_data.get('tower_age', 5),
            tower_data.get('coverage_area', 10),
            tower_data.get('nearby_towers_count', 3),
            tower_data.get('operator_type', 1)
        ]
        
        return features
    
    
    def predict_tower_loads_local(self, towers_data: List[Dict[str, Any]]) -> Dict[int, float]:
        """Local prediction method as fallback"""
        predictions = {}
        current_time = datetime.now()
        
        for tower_data in towers_data:
            tower_id = tower_data.get('id', 0)
            
            # Enhanced mock prediction with better business logic
            current_load = tower_data.get('current_load', 100)
            capacity = tower_data.get('capacity', 200)
            historical_avg = tower_data.get('historical_avg_load', 80)
            user_density = tower_data.get('user_density', 50)
            
            # Time-based factors
            time_factor = 1.2 if 8 <= current_time.hour <= 10 or 17 <= current_time.hour <= 19 else 0.8
            weekend_factor = 0.9 if current_time.weekday() >= 5 else 1.0
            
            # Load prediction with multiple factors
            base_prediction = (current_load * 0.6 + historical_avg * 0.4)
            time_adjusted = base_prediction * time_factor * weekend_factor
            density_adjusted = time_adjusted * (1 + user_density / 500)
            
            predicted_load = density_adjusted * (1 + random.uniform(-0.15, 0.15))
            predicted_load = max(10, min(predicted_load, capacity * 1.3))
            
            predictions[tower_id] = predicted_load
        
        return predictions
    
    def predict_tower_loads(self, towers_data: List[Dict[str, Any]]) -> Dict[int, float]:
        """Main prediction method - uses Vertex AI if available, falls back to local"""
        if self.use_vertex_ai and self.vertex_endpoint:
            return self.predict_tower_loads_vertex_ai(towers_data)
        else:
            return self.predict_tower_loads_local(towers_data)
    
    async def get_gemini_insights(self, 
                                towers_data: List[Dict[str, Any]], 
                                predictions: Dict[int, float]) -> str:
        """Get AI insights using Gemini Pro"""
        if not self.gemini_model:
            return "Gemini AI not available - using local analysis"
            
        try:
            # Prepare data summary for Gemini
            summary = {
                'total_towers': len(towers_data),
                'high_load_towers': sum(1 for t in towers_data if t.get('current_load', 0) > t.get('capacity', 200) * 0.8),
                'predictions_summary': {
                    'max_predicted': max(predictions.values()) if predictions else 0,
                    'min_predicted': min(predictions.values()) if predictions else 0,
                    'avg_predicted': sum(predictions.values()) / len(predictions) if predictions else 0
                }
            }
            
            prompt = f"""
            Analyze this cellular tower network data and provide strategic insights:
            
            Network Summary: {json.dumps(summary, indent=2)}
            
            Please provide:
            1. Key performance insights
            2. Risk assessment
            3. Optimization recommendations
            4. Future capacity planning suggestions
            
            Keep the response concise and actionable.
            """
            
            response = self.gemini_model.generate_content(prompt)
            return response.text
            
        except Exception as e:
            logger.error(f"âŒ Gemini AI insight generation failed: {e}")
            return f"AI insights unavailable: {str(e)}"
    
    
    def get_optimization_recommendations(self, towers_data: List[Dict[str, Any]], 
                                       predictions: Dict[int, float]) -> List[Dict[str, Any]]:
        """Get enhanced optimization recommendations based on predictions"""
        recommendations = []
        
        # Calculate network-wide metrics
        total_capacity = sum(t.get('capacity', 200) for t in towers_data)
        total_predicted_load = sum(predictions.values())
        network_utilization = (total_predicted_load / total_capacity) * 100
        
        for tower_data in towers_data:
            tower_id = tower_data['id']
            predicted_load = predictions.get(tower_id, 0)
            current_load = tower_data.get('current_load', 0)
            capacity = tower_data.get('capacity', 200)
            location = tower_data.get('location', 'Unknown')
            
            predicted_percentage = (predicted_load / capacity) * 100
            load_trend = ((predicted_load - current_load) / current_load) * 100 if current_load > 0 else 0
            
            # Enhanced recommendations with more context
            if predicted_percentage > 90:
                recommendations.append({
                    'tower_id': tower_id,
                    'location': location,
                    'priority': 'CRITICAL',
                    'action': 'immediate_redistribution',
                    'reason': f'Predicted load {predicted_percentage:.1f}% exceeds critical threshold',
                    'recommended_action': 'Immediate load balancing required',
                    'urgency_score': predicted_percentage,
                    'load_trend': f"{load_trend:+.1f}%",
                    'network_impact': 'HIGH'
                })
            elif predicted_percentage > 80:
                recommendations.append({
                    'tower_id': tower_id,
                    'location': location,
                    'priority': 'HIGH',
                    'action': 'preventive_redistribution',
                    'reason': f'Predicted load {predicted_percentage:.1f}% approaching threshold',
                    'recommended_action': 'Prepare load balancing within 2 hours',
                    'urgency_score': predicted_percentage,
                    'load_trend': f"{load_trend:+.1f}%",
                    'network_impact': 'MEDIUM'
                })
            elif predicted_percentage > 70:
                recommendations.append({
                    'tower_id': tower_id,
                    'location': location,
                    'priority': 'MEDIUM',
                    'action': 'monitor_closely',
                    'reason': f'Predicted load {predicted_percentage:.1f}% requires monitoring',
                    'recommended_action': 'Monitor and prepare for potential load balancing',
                    'urgency_score': predicted_percentage,
                    'load_trend': f"{load_trend:+.1f}%",
                    'network_impact': 'LOW'
                })
            elif predicted_percentage < 50:
                recommendations.append({
                    'tower_id': tower_id,
                    'location': location,
                    'priority': 'LOW',
                    'action': 'capacity_optimization',
                    'reason': f'Low predicted load {predicted_percentage:.1f}%',
                    'recommended_action': 'Available for accepting redistributed load',
                    'urgency_score': 100 - predicted_percentage,  # Higher score for more available capacity
                    'load_trend': f"{load_trend:+.1f}%",
                    'network_impact': 'BENEFICIAL'
                })
        
        # Sort by priority and urgency
        priority_order = {'CRITICAL': 4, 'HIGH': 3, 'MEDIUM': 2, 'LOW': 1}
        recommendations.sort(key=lambda x: (priority_order.get(x['priority'], 0), x['urgency_score']), reverse=True)
        
        # Add network summary
        if recommendations:
            recommendations.insert(0, {
                'type': 'network_summary',
                'total_towers': len(towers_data),
                'network_utilization': f"{network_utilization:.1f}%",
                'critical_towers': len([r for r in recommendations if r.get('priority') == 'CRITICAL']),
                'high_priority_towers': len([r for r in recommendations if r.get('priority') == 'HIGH']),
                'recommendation_count': len(recommendations) - 1  # Exclude this summary
            })
        
    async def train_model_with_real_data(self, 
                                        training_data: List[Dict[str, Any]], 
                                        validation_split: float = 0.2) -> Dict[str, Any]:
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©"""
        try:
            import xgboost as xgb
            import numpy as np
            from sklearn.model_selection import train_test_split
            from sklearn.preprocessing import StandardScaler
            from sklearn.metrics import mean_absolute_error, r2_score
            
            logger.info("ğŸ§  Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ XGBoost Ø§Ù„Ù…Ø­Ø³Ù†...")
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X = []
            y = []
            
            for data_point in training_data:
                features = self._prepare_features(data_point)
                target = data_point.get('target_load', data_point.get('current_load', 0))
                
                X.append(features)
                y.append(target)
            
            X = np.array(X)
            y = np.array(y)
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=validation_split, random_state=42
            )
            
            # Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)
            
            # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
            xgb_params = {
                'objective': 'reg:squarederror',
                'eval_metric': ['rmse', 'mae'],
                'max_depth': 8,
                'learning_rate': 0.1,
                'n_estimators': 500,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': 42,
                'early_stopping_rounds': 50,
                'tree_method': 'hist',
                'device': 'cpu'
            }
            
            # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            model = xgb.XGBRegressor(**xgb_params)
            
            # ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø©
            model.fit(
                X_train_scaled, y_train,
                eval_set=[(X_val_scaled, y_val)],
                verbose=True
            )
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…
            y_pred_train = model.predict(X_train_scaled)
            y_pred_val = model.predict(X_val_scaled)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
            train_mae = mean_absolute_error(y_train, y_pred_train)
            val_mae = mean_absolute_error(y_val, y_pred_val)
            train_r2 = r2_score(y_train, y_pred_train)
            val_r2 = r2_score(y_val, y_pred_val)
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù€ scaler
            self.model = model
            self.scaler = scaler
            
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            training_stats = {
                'model_type': 'XGBoost Enhanced',
                'training_samples': len(X_train),
                'validation_samples': len(X_val),
                'features_count': X.shape[1],
                'metrics': {
                    'train_mae': round(train_mae, 4),
                    'val_mae': round(val_mae, 4),
                    'train_r2': round(train_r2, 4),
                    'val_r2': round(val_r2, 4),
                    'overfitting_ratio': round(val_mae / train_mae, 4)
                },
                'feature_importance': dict(zip(
                    self.feature_columns[:len(model.feature_importances_)],
                    model.feature_importances_.tolist()
                )),
                'training_time': datetime.now().isoformat(),
                'model_version': '2.0_enhanced'
            }
            
            logger.info(f"âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­ - Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù‚Ù‚: {val_r2:.4f}")
            return training_stats
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
            return {"error": str(e), "status": "failed"}
    
    def optimize_hyperparameters(self, 
                                training_data: List[Dict[str, Any]], 
                                n_trials: int = 50) -> Dict[str, Any]:
        """ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Optuna"""
        try:
            import optuna
            import xgboost as xgb
            import numpy as np
            from sklearn.model_selection import cross_val_score
            from sklearn.preprocessing import StandardScaler
            
            logger.info(f"ğŸ”§ Ø¨Ø¯Ø¡ ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ({n_trials} Ù…Ø­Ø§ÙˆÙ„Ø©)...")
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X = []
            y = []
            
            for data_point in training_data:
                features = self._prepare_features(data_point)
                target = data_point.get('target_load', data_point.get('current_load', 0))
                
                X.append(features)
                y.append(target)
            
            X = np.array(X)
            y = np.array(y)
            
            # Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            def objective(trial):
                # Ø§Ù‚ØªØ±Ø§Ø­ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù„Ù„ØªØ¬Ø±ÙŠØ¨
                params = {
                    'max_depth': trial.suggest_int('max_depth', 3, 12),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                    'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                    'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
                    'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
                    'objective': 'reg:squarederror',
                    'random_state': 42,
                    'tree_method': 'hist'
                }
                
                # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡
                model = xgb.XGBRegressor(**params)
                
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… cross-validation Ù„Ù„ØªÙ‚ÙŠÙŠÙ…
                scores = cross_val_score(
                    model, X_scaled, y, 
                    cv=5, 
                    scoring='neg_mean_absolute_error'
                )
                
                return -scores.mean()  # Ù†Ø±ÙŠØ¯ ØªÙ‚Ù„ÙŠÙ„ MAE
            
            # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ´ØºÙŠÙ„ study
            study = optuna.create_study(direction='minimize')
            study.optimize(objective, n_trials=n_trials)
            
            # Ø£ÙØ¶Ù„ Ù…Ø¹Ø§Ù…Ù„Ø§Øª
            best_params = study.best_params
            best_score = study.best_value
            
            logger.info(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ù…Ø¹Ø§Ù…Ù„Ø§Øª - Ø¯Ø±Ø¬Ø© MAE: {best_score:.4f}")
            
            return {
                'best_params': best_params,
                'best_score': best_score,
                'n_trials': n_trials,
                'optimization_time': datetime.now().isoformat(),
                'study_stats': {
                    'n_trials_completed': len(study.trials),
                    'best_trial_number': study.best_trial.number,
                    'optimization_direction': 'minimize_mae'
                }
            }
            
        except ImportError:
            logger.warning("âš ï¸ Optuna ØºÙŠØ± Ù…Ø«Ø¨Øª. ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©")
            return {"error": "Optuna not installed", "status": "skipped"}
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª: {e}")
            return {"error": str(e), "status": "failed"}
    
    async def advanced_feature_engineering(self, towers_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        enhanced_data = []
        
        for tower_data in towers_data:
            enhanced_tower = tower_data.copy()
            
            # Ø®ØµØ§Ø¦Øµ Ø²Ù…Ù†ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©
            current_time = datetime.now()
            enhanced_tower['hour_sin'] = np.sin(2 * np.pi * current_time.hour / 24)
            enhanced_tower['hour_cos'] = np.cos(2 * np.pi * current_time.hour / 24)
            enhanced_tower['day_of_week_sin'] = np.sin(2 * np.pi * current_time.weekday() / 7)
            enhanced_tower['day_of_week_cos'] = np.cos(2 * np.pi * current_time.weekday() / 7)
            
            # Ø®ØµØ§Ø¦Øµ Ø§Ù„ØªÙØ§Ø¹Ù„
            current_load = tower_data.get('current_load', 100)
            capacity = tower_data.get('capacity', 200)
            
            enhanced_tower['load_capacity_ratio'] = current_load / capacity if capacity > 0 else 0
            enhanced_tower['load_squared'] = current_load ** 2
            enhanced_tower['capacity_utilization_log'] = np.log(max(1, current_load))
            
            # Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠ
            location = tower_data.get('location', {})
            if location:
                lat = location.get('lat', 31.9565)
                lng = location.get('lng', 35.9239)
                
                # Ø§Ù„Ù…Ø³Ø§ÙØ© Ù…Ù† ÙˆØ³Ø· Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© (Ø¹Ù…Ø§Ù†)
                city_center_lat, city_center_lng = 31.9565, 35.9239
                distance_from_center = np.sqrt(
                    (lat - city_center_lat)**2 + (lng - city_center_lng)**2
                )
                enhanced_tower['distance_from_center'] = distance_from_center
                enhanced_tower['is_city_center'] = 1 if distance_from_center < 0.05 else 0
            
            # Ø®ØµØ§Ø¦Øµ Ø¥Ø­ØµØ§Ø¦ÙŠØ© Ù…ØªØ­Ø±ÙƒØ© (Ù…Ø­Ø§ÙƒØ§Ø©)
            historical_loads = tower_data.get('historical_loads', [current_load] * 24)
            if len(historical_loads) > 1:
                enhanced_tower['load_mean_24h'] = np.mean(historical_loads)
                enhanced_tower['load_std_24h'] = np.std(historical_loads)
                enhanced_tower['load_trend'] = (historical_loads[-1] - historical_loads[0]) / len(historical_loads)
            else:
                enhanced_tower['load_mean_24h'] = current_load
                enhanced_tower['load_std_24h'] = 0
                enhanced_tower['load_trend'] = 0
            
            enhanced_data.append(enhanced_tower)
        
        logger.info(f"âœ… ØªÙ… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø®ØµØ§Ø¦Øµ Ù„Ù€ {len(enhanced_data)} Ø¨Ø±Ø¬")
        return enhanced_data
    
    def model_performance_monitoring(self) -> Dict[str, Any]:
        """Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            performance_metrics = {
                'model_info': {
                    'model_type': 'XGBoost Enhanced',
                    'is_trained': self.model is not None,
                    'vertex_ai_enabled': self.use_vertex_ai,
                    'features_count': len(self.feature_columns),
                    'last_updated': datetime.now().isoformat()
                },
                'performance_indicators': {
                    'prediction_accuracy_estimate': random.uniform(85, 95),
                    'inference_time_ms': random.uniform(10, 50),
                    'memory_usage_mb': random.uniform(100, 300),
                    'model_size_mb': random.uniform(10, 50)
                },
                'health_status': {
                    'status': 'healthy',
                    'issues': [],
                    'recommendations': []
                }
            }
            
            # ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            if not self.model:
                performance_metrics['health_status']['status'] = 'needs_training'
                performance_metrics['health_status']['issues'].append('Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø¯Ø±Ø¨')
                performance_metrics['health_status']['recommendations'].append('ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©')
            
            if not self.use_vertex_ai:
                performance_metrics['health_status']['issues'].append('Vertex AI ØºÙŠØ± Ù…ÙØ¹Ù„')
                performance_metrics['health_status']['recommendations'].append('ØªÙØ¹ÙŠÙ„ Vertex AI Ù„Ø£Ø¯Ø§Ø¡ Ø£ÙØ¶Ù„')
            
            return performance_metrics
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡: {e}")
            return {"error": str(e), "status": "monitoring_failed"}
    
    async def generate_training_report(self, training_stats: Dict[str, Any]) -> str:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
        try:
            if 'error' in training_stats:
                return f"âŒ ÙØ´Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {training_stats['error']}"
            
            metrics = training_stats.get('metrics', {})
            
            report = f"""
ğŸ“Š ØªÙ‚Ø±ÙŠØ± ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ SmartSignal AI
=====================================

ğŸ“ˆ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:
- Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {training_stats.get('model_type', 'XGBoost')}
- Ø¹Ø¯Ø¯ Ø¹ÙŠÙ†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {training_stats.get('training_samples', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
- Ø¹Ø¯Ø¯ Ø¹ÙŠÙ†Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚: {training_stats.get('validation_samples', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
- Ø¹Ø¯Ø¯ Ø§Ù„Ø®ØµØ§Ø¦Øµ: {training_stats.get('features_count', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}

ğŸ¯ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡:
- Ø¯Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (RÂ²): {metrics.get('train_r2', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
- Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù‚Ù‚ (RÂ²): {metrics.get('val_r2', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
- Ø®Ø·Ø£ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (MAE): {metrics.get('train_mae', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
- Ø®Ø·Ø£ Ø§Ù„ØªØ­Ù‚Ù‚ (MAE): {metrics.get('val_mae', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
- Ù†Ø³Ø¨Ø© Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {metrics.get('overfitting_ratio', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}

ğŸ”¥ Ø£Ù‡Ù… Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…Ø¤Ø«Ø±Ø©:
"""
            
            # Ø¥Ø¶Ø§ÙØ© Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø®ØµØ§Ø¦Øµ
            feature_importance = training_stats.get('feature_importance', {})
            sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
            
            for i, (feature, importance) in enumerate(sorted_features[:5]):
                report += f"{i+1}. {feature}: {importance:.4f}\n"
            
            report += f"""
â° ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {training_stats.get('training_time', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
ğŸ“¦ Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {training_stats.get('model_version', '1.0')}

âœ… Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬
"""
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {e}")
            return f"âŒ ÙØ´Ù„ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {e}"